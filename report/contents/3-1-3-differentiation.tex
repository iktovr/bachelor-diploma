\subsubsection{Численное дифференцирование}

\paragraph{Конечно-разностный метод}

Простейшими методами численного дифференцирования является семейство методов, использующих конечные разности. В таких методах оператор дифференцирования аппроксимируется отношением конечных разностей, которое получается путем дифференцирования аппроксимирующего многочлена. Используя многочлены различных степеней, можно получить конечно-разностные формулы соответствующего порядка аппроксимации.

Пусть функция $f(x)$ задана таблично $f_i = f(t_i),\ i = \overline{1, n}$ на регулярной сетке с шагом $x_{i+1} - x_i = h$. Тогда для вычисления производной первого порядка в тех же точках $f'_i = f'(t_i),\ i = \overline{1, n}$ будем использовать следующие конечно-разностные формулы:

\begin{align}
f'_1 &= \frac{-3 f_1 + 4 f_2 - f_3}{2 h} \\
f'_i &= \frac{f_{i+1} - f_{i-1}}{2 h}, \qquad i = \overline{2, n-1} \\
f'_n &= \frac{f_{n-2} - 4 f_{n-1} + 3 f_n}{2 h}.
\end{align}

Такие формулы обеспечивают второй порядок аппроксимации.

Значительным недостатком конечно-разностного метода является то, что при наличии шумовой составляющей в используемых данных $x_i$, шумовая составляющая усиливается, что приводит к некорректным значениям производной. Таким образом, с точки зрения практического использования этот метод имеет очень узкую область применимости.

\paragraph{Регуляризация полной вариации}

Более устойчивый к шуму метод можно получить, используя регуляризацию самого процесса дифференцирования, а именно --- регуляризацию полной вариации.

Полная вариация является обобщением понятия длины кривой, задаваемой произвольной функцией $f: [a, b] \rightarrow \mathbb{R}^n$. Если $f \in C^1$, то есть имеет непрерывную первую производную, $f$ является функцией ограниченной вариации, которая вычисляется по формуле:

\begin{equation}
\int\limits_a^b ||f'(x)||\,\mathrm{d}(x),
\end{equation}

где $||\cdot||$ --- некоторая норма в пространстве $\mathbb{R}^n$.

Задачу дифференцирования функции $f' = u$ на отрезке $[0, L]$ можно свести к интегральному уравнению Вольтерра первого рода:

\begin{equation}
\int_0^x u(s)\,\mathrm{d}(s) = f(x) - f(0),
\end{equation}

где $x \in [0, L]$.

Обозначим $A u (x) = \int_0^x u$ --- оператор интегрирования, также предположим, что $f(0) = 0$ (на практике это достигается вычитанием $f(0)$ из $f$):

\begin{equation}
A u = f.
\end{equation}

Это уравнение относится к некорректным в силу высокой чувствительности решения к малым возмущениям правой части. Непосредственное решение данного уравнения не имеет смысла. Вместо этого будем решать задачу минимизации функционала:

\begin{equation}
F(u) = DF(A u - f) + \alpha R \longrightarrow \min_u,
\end{equation}

где $DF(A u - f)$ --- компонента, отвечающая за точность решения;
\par $\alpha$ --- коэффициент регуляризации;
\par $R$ --- регуляризационная компонента.

Функционал $DF$ обычно представляет из себя квадрат $L^2$ нормы, $DF = \int_0^L |\cdot|^2$, так как предполагается, что $f$ имеет в своем составе белый гауссов шум. Для шумов с другим распределением необходимо использовать другие функционалы.

С этого момента существует несколько подходов к решение этой задачи, связанных с разными формами регуляризации.

\subparagraph{Первый метод}

Первый способ заключается в регуляризации --- в виде квадрата $L^2$ нормы --- производной функции $f$ \cite{fast_tvr}:

\begin{equation}
R = \int\limits_0^L |f'|^2.
\end{equation}

Таким образом происходит регуляризация полной вариации $f$ и функционал $F$ принимает вид:

\begin{equation}
F(u) = \int\limits_0^L |A u - f|^2 + \alpha \int\limits_0^L |f'|.
\end{equation}

Аппроксимируя операторы интегрирования и дифференцирования, можно переписать задачу в матрично-векторной форме (о конкретном виде дискретных аналогов операторов будет сказано позже):

\begin{equation}
F(u) = ||A u - f||_2^2 + \alpha ||D f||_2^2,
\end{equation}

где $u$ --- искомый вектор производных,\par 
$f$ --- вектор приращений функции,\par 
$A$ --- матричный оператор интегрирования,\par 
$D$ --- матричный оператор дифференцирования,\par 
$||\cdot||_2$ --- евклидова норма вектора.

Минимизируемая функция квадратичная, следовательно задача минимизации может быть решена с использованием необходимого условия экстремума:

\begin{equation}
\nabla F(u) = 0.
\end{equation}

В результате задача сводится к решению системы линейных алгебраических уравнений относительно вектора $u$:

\begin{equation}
B u = b,
\end{equation}

где $B = A^T A + \alpha D^T D$,\par $b = A^T u$.

\subparagraph{Второй метод}

Для второго способа вместо квадрата $L^2$ нормы используем $H^1_1$ норму искомой производной $u$ \cite{tvr}:

\begin{equation}
R = \int\limits_0^L |u'|.
\end{equation}

Тогда регуляризации подвергается полная вариация $u$ и функционал $F$ имеет вид:

\begin{equation}
F(u) = \int\limits_0^L |A u - f|^2 + \alpha \int\limits_0^L |u'|.
\end{equation}

Такую задачу минимизации уже нельзя решить, используя только необходимое условие экстремума, и необходимо применять другие методы оптимизации. Простейшим способом будет использовать метод градиентного спуска. Для такой задачи этот метод сводится к эволюции дифференциального уравнения до стационарного состояния:

\begin{equation}
u_t = \alpha \frac{\mathrm{d}}{\mathrm{d}x} \frac{u'}{|u'|} - A^T (A u - f),
\end{equation}

где $A^T v(x) = \int_x^L v$ --- сопряженный оператор к $A$.

Во избежании деления на ноль, $|u'|$ в знаменателе можно заменить на $\sqrt{(u')^2 + \varepsilon}$, для некоторого малого $\varepsilon > 0$.

Проблема градиентного спуска состоит в медленной сходимости. Поэтому вместо него предлагается использовать метод запаздывающей диффузии (lagged diffusivity) \cite{lagged_diff_1, lagged_diff_2}. Основная идея этого метода заключается в замене нелинейного дифференциального оператора $u \mapsto (\mathrm{d} / \mathrm{d}x)(u'/|u'|)$ линейным оператором $u \mapsto (\mathrm{d} / \mathrm{d}x)(u'/|u'_k|)$.

Для этого необходимо произвести дискретизацию. Предполагаем, что функция $f$ задана на регулярной сетке с шагом $h$ в точках $\{x_i\}_0^n = \{0, h, 2h, \ldots, L\}$, ее производную $u$ будем искать в точках $\{x_i\}_0^n \cup \{x_{-1}\} = \{-h, 0, h, 2h, \ldots, L\}$ (первая точка нужна только для работы алгоритма и в результат не входит). Производную $u$ будем вычислять между точками сетки, как центральные разности:

\begin{equation}
D u(x_i + \frac{h}{2}) = \frac{u(x_{i+1} - u(x_i))}{h}, \qquad i = \overline{-1, n-1}. 
\end{equation}

Интеграл от $u$ будем вычислять в точках $\{x_i\}_0^n$, используя метод трапеций:

\begin{equation}
A u(x_i) = \displaystyle\sum_{j=-1}^{i-1} \frac{u(x_j) + u(x_{j+1})}{2} h, \qquad i = \overline{0, n}.
\end{equation}

Таким образом, матрицы операторов интегрирования и дифференцирования, обе размером $n \times (n + 1)$, выглядят следующим образом:   

\begin{equation}
D_{n, n+1} = \frac{1}{h}
\begin{pmatrix}
-1 &  1 &  0 & 0 & \cdots &  0 &  0 &  0 \\
 0 & -1 &  1 & 0 & \cdots &  0 &  0 &  0 \\
 0 &  0 & -1 & 1 & \cdots &  0 &  0 &  0 \\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
 0 &  0 &  0 & 0 & \cdots & -1 &  1 &  0 \\
 0 &  0 &  0 & 0 & \cdots &  0 & -1 &  1 
\end{pmatrix}
\end{equation}

\begin{equation}
A_{n, n+1} = h
\begin{pmatrix}
0.5 & 0.5 &   0 &   0 & \cdots & 0 &   0 &   0 \\
0.5 &   1 & 0.5 &   0 & \cdots & 0 &   0 &   0 \\
0.5 &   1 &   1 & 0.5 & \cdots & 0 &   0 &   0 \\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
0.5 &   1 &   1 &   1 & \cdots & 1 & 0.5 &   0 \\
0.5 &   1 &   1 &   1 & \cdots & 1 &   1 & 0.5 
\end{pmatrix}.
\end{equation}

Подобные матрицы и подход с дополнительной точкой можно также использовать и в первом методе.

Метод запаздывающей диффузии представляет из себя итеративный процесс. На каждой итерации $k$ используются следующие матрицы:

\begin{align}
E_k &= \operatorname{diag}(((D u_k)^2 + \varepsilon)^{-1/2}),\\
L_k &= h D^T E_k D,\\
H_k &= A^T A + \alpha L_k,
\end{align}

где $\operatorname{diag}(v)$ --- диагональная матрица со значениями вектора $v$ на главной диагонали;\par
$H_k$ --- аппроксимация гессиана $F$ в точке $u_k$.

Переход к следующей итерации:

\begin{align}
& g_k = A^T (A u_k - f) + \alpha L_k u_k, \\
\label{eq:s}
& H_k s_k = g_k, \\
& u_{k+1} = u_k + s_k,
\end{align}

где $s_k$ --- решение системы линейных алгебраических уравнений~(\ref{eq:s}).

Начальным значением $u$ может быть нулевой вектор или вектор производных, полученных методом конечных разностей.
Завершение итеративного процесса можно произвести по достижению заданного числа итераций, или при незначительном изменении вектора $u_k$, относительно предыдущего значения. Псевдокод алгоритма представлен на рисунке~\ref{alg:tvr}.

\begin{figure}
\begin{minipage}{\linewidth}
\begin{algorithm}[H]
\SetAlgoVlined
\KwData{коэффициент регуляризации $\alpha$, число итераций $K$}
\KwIn{вектор значений функции $f$, шаг регулярной сетки $h$}
\KwOut{вектор производных $u$}

$n = \text{размер вектора}\ f$\;
$u = {\underbrace{(0 \ 0 \ \cdots \ 0)}_{n+1}}^T$\;
\For{$k = 1$ \KwTo $K$}{
    $E = \operatorname{diag}(((D u)^2 + \varepsilon)^{-1/2})$\;
    $L = h D^T E D$\;
    $H = A^T A + \alpha L$\;
    $g = A^T (A u - f) + \alpha L u$\;
    $s = \operatorname{solve}(H, g)$ \tcp*[l]{функция solve решает СЛАУ}
    $u = u + s$\;
}
\KwRet{$(u_1 \ u_2 \ \cdots \ u_{n+1})^T$}
\end{algorithm}
\end{minipage}
\caption{Псевдокод второго метода алгоритма дифференцирования}
\label{alg:tvr}
\end{figure}
